{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cce6cbd-905c-4a8a-8204-c01eaa19843a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Case Taxis NY\n",
    "Tecnologias implementadas:\n",
    "    - Arquitetura medalhao\n",
    "    - Particionamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7d79e3-9b0b-41dd-b92a-93eae33a345c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configurando conexao com AWS S3 Bucket - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df13b5fc-5c52-43c2-a0b1-533aeae28905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (2025.9.0)\n",
      "Requirement already satisfied: s3fs in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (2025.9.0)\n",
      "Requirement already satisfied: boto3 in /databricks/python3/lib/python3.12/site-packages (1.34.69)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from s3fs) (2.13.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from s3fs) (3.12.15)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.69 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from boto3) (1.34.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.12/site-packages (from boto3) (0.10.4)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /databricks/python3/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.14.1)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1cf53efa-4123-4605-9f32-84a40bc03554/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.12/site-packages (from botocore<1.35.0,>=1.34.69->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /databricks/python3/lib/python3.12/site-packages (from botocore<1.35.0,>=1.34.69->boto3) (2.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /databricks/python3/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.69->boto3) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in /databricks/python3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.7)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fsspec s3fs boto3\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052afd38-cc71-4bd4-8466-a71f4e8848b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connecting to S3 Bucket\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cred = pd.read_csv('databricks_user_accessKeys.csv') # Arquivo gerado automaticamente pelo AWS IAM\n",
    "\n",
    "access_key = cred['access_key_ID'].iloc[0]\n",
    "secret_key = cred['secret_access_key'].iloc[0]\n",
    "bucket = \"ny-taxi-case\"\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = access_key\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = secret_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a96653-1875-49fb-8f53-7aa4b097725d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingestao de dados brutos\n",
    "Nesta etapa, os dados brutos serao baixados da base oficial da agencia responsavel pelo licenciamento dos taxis em NY enviados para o S3 bucket criado para o projeto. \n",
    "Utilizou-se uma funcao de repeticao para automatizar o processo de aquisicao e ingestao dos dados, ja que os link para os arquivos respeitavam o seguinte padrao:\n",
    "https://d37ci6vzurychx.cloudfront.net/trip-data/[TIPO_DE_TAXI]\\_tripdata\\_[ANO]-[MES].parquet\n",
    "\n",
    "Os arquivos serao organizados no S3 bucket com o seguinte padrao:\n",
    "raw_data/[TIPO_DE_TAXI]/[ANO]/[MES]/arquivo.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be759a83-61e2-4efc-8a17-3e319362abf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Funcao auxiliar para manipulacao de datas na proxima etapa\n",
    "def create_month_year_array(start_date, end_date):\n",
    "    start_month, start_year = start_date\n",
    "    end_month, end_year = end_date\n",
    "    \n",
    "    date_list = []\n",
    "    current_month = start_month\n",
    "    current_year = start_year\n",
    "\n",
    "    while True:\n",
    "        # Format the month with a leading zero if it's less than 10\n",
    "        formatted_month = f\"{current_month:02d}\"\n",
    "        date_list.append([formatted_month, current_year])\n",
    "\n",
    "        if current_month == end_month and current_year == end_year:\n",
    "            break\n",
    "\n",
    "        current_month += 1\n",
    "        if current_month > 12:\n",
    "            current_month = 1\n",
    "            current_year += 1\n",
    "            \n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "959edb44-ea67-4ea7-b823-805f83e84783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download de yellow_tripdata_2023-01.parquet concluido\n",
      "Upload de yellow_tripdata_2023-01.parquet para S3 concluido\n",
      "Download de green_tripdata_2023-01.parquet concluido\n",
      "Upload de green_tripdata_2023-01.parquet para S3 concluido\n",
      "Download de yellow_tripdata_2023-02.parquet concluido\n",
      "Upload de yellow_tripdata_2023-02.parquet para S3 concluido\n",
      "Download de green_tripdata_2023-02.parquet concluido\n",
      "Upload de green_tripdata_2023-02.parquet para S3 concluido\n",
      "Download de yellow_tripdata_2023-03.parquet concluido\n",
      "Upload de yellow_tripdata_2023-03.parquet para S3 concluido\n",
      "Download de green_tripdata_2023-03.parquet concluido\n",
      "Upload de green_tripdata_2023-03.parquet para S3 concluido\n",
      "Download de yellow_tripdata_2023-04.parquet concluido\n",
      "Upload de yellow_tripdata_2023-04.parquet para S3 concluido\n",
      "Download de green_tripdata_2023-04.parquet concluido\n",
      "Upload de green_tripdata_2023-04.parquet para S3 concluido\n",
      "Download de yellow_tripdata_2023-05.parquet concluido\n",
      "Upload de yellow_tripdata_2023-05.parquet para S3 concluido\n",
      "Download de green_tripdata_2023-05.parquet concluido\n",
      "Upload de green_tripdata_2023-05.parquet para S3 concluido\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import boto3\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "start_date = [1, 2023] # Janeiro de 2023\n",
    "end_date = [5, 2023] # Maio de 2023\n",
    "date_list = create_month_year_array(start_date, end_date)\n",
    "trip_data_list=[\"yellow\", \"green\"]\n",
    "s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key,  region_name='us-east-2')\n",
    "bucket = \"ny-taxi-case\"\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "remote_path_list = []\n",
    "\n",
    "for date in date_list:\n",
    "\n",
    "    month = date[0]\n",
    "    year = date[1]\n",
    "\n",
    "    for trip_data in trip_data_list:\n",
    "        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{trip_data}_tripdata_{year}-{month}.parquet\"\n",
    "        local_path = f\"data/{trip_data}_tripdata_{year}-{month}.parquet\"\n",
    "        remote_path = f\"raw_data/{trip_data}/{year}/{month}/{trip_data}-{year}-{month}.parquet\"\n",
    "\n",
    "        urllib.request.urlretrieve(url, local_path)\n",
    "        print(f\"Download de {trip_data}_tripdata_{year}-{month}.parquet concluido\")\n",
    "        s3.upload_file(local_path, bucket, remote_path)\n",
    "        print(f\"Upload de {trip_data}_tripdata_{year}-{month}.parquet para S3 concluido\")\n",
    "\n",
    "        remote_path_list.append(remote_path)\n",
    "shutil.rmtree('data') # Remove arquivos locais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e51ad0-5c03-40dc-b105-21918058f787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Refinamento dos dados\n",
    "Nesta etapa sera feita a limpeza e organizacao dos dados. Serao selecionados apenas colunas de interesse, remocao de dados nulos, exclusao de registros duplicados ou incoerentes: (corrida fora do intervalo de interesse, com valor recebido menor ou igual 0...), alem da padronizacao do tipo de cada coluna dos parquets. \n",
    "\n",
    "Os arquivos serao organizados no S3 bucket com o seguinte padrao:\n",
    "cleaned_data/[TIPO_DE_TAXI]/[ANO]/[MES]/arquivo.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c0a73e2-27a3-4c92-8750-80cfc3ba8f34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "for date in date_list:\n",
    "\n",
    "  month = date[0]\n",
    "  year = date[1]\n",
    "\n",
    "  for trip_data in trip_data_list:\n",
    "    relative_path = f\"raw_data/{trip_data}/{year}/{month}/{trip_data}-{year}-{month}.parquet\"\n",
    "    df = spark.read.format(\"parquet\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(f\"s3://{bucket}/{relative_path}\")\n",
    "\n",
    "    # Renomeia coluna para caso dos Green Taxis\n",
    "    df = (df.withColumnRenamed(\"lpep_pickup_datetime\", \"tpep_pickup_datetime\")\n",
    "            .withColumnRenamed(\"lpep_dropoff_datetime\", \"tpep_dropoff_datetime\")\n",
    "        )    \n",
    "    \n",
    "    # Seleciona colunas de interesse, remove nulos, registros duplicados e filtra incoerencias\n",
    "    df = (df.select([\"VendorID\",\"passenger_count\", \"total_amount\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\"])\n",
    "            .dropna()\n",
    "            .dropDuplicates()\n",
    "            .filter(df['passenger_count'] > 0) # Corrida com zero passagerios\n",
    "            .filter(df['total_amount'] > 0) # Corrida com valor total negativo\n",
    "            .filter(df['tpep_pickup_datetime'] < df['tpep_dropoff_datetime']) # Tempo de chegada < tempo de partida\n",
    "            .filter(df['tpep_pickup_datetime'] > F.to_timestamp(F.lit(f\"{year}-{month}-01\"))) # Corrida fora do mes\n",
    "            .filter(df['tpep_pickup_datetime'] < F.add_months(F.to_timestamp(F.lit(f\"{year}-{month}-01\")), 1)) # Corrida fora do mes\n",
    "        )\n",
    "    \n",
    "    # Garante que a mesma coluna tenha o mesmo tipo nos diferentes parquets\n",
    "    df = (df.withColumn(\"VendorID\", F.col(\"VendorID\").cast(\"long\"))\n",
    "          .withColumn(\"passenger_count\", F.col(\"passenger_count\").cast(\"int\"))\n",
    "          .withColumn(\"total_amount\", F.col(\"total_amount\").cast(\"double\"))\n",
    "          .withColumn(\"tpep_pickup_datetime\", F.col(\"tpep_pickup_datetime\").cast(\"timestamp\"))\n",
    "          .withColumn(\"tpep_dropoff_datetime\", F.col(\"tpep_dropoff_datetime\").cast(\"timestamp\"))\n",
    "      )\n",
    "    \n",
    "    # Upload de dado tratado para Bucket S3\n",
    "    s3_path = f\"s3://{bucket}/{relative_path.replace(\"raw_data\", \"cleaned_data\")}\"\n",
    "    df.write.parquet(s3_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653ed6a9-0ee4-4390-aa79-f2c136930736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Dados de metricas\n",
    "Por fim, para agilizar e otimizar consultas para geracao de BIs, dashes, ou reports, foi tambem gerado um arquivo de metricas. Para este caso, os dados foram separados e estruturados trazendo um resumo dos registros por hora, auziliando assim uma analise por tempo dos registros. \n",
    "O arquivo de metricas foi estruturado com as seguintes colunuas: date, hour, sum_total_amount, sum_passenger_count, total_rides, avg_total_amounts e avg_passenger_count.\n",
    "\n",
    "Utilizou-se funcoes PySpark para manipulacao dos dados. \n",
    "\n",
    "Os arquivos serao organizados no S3 bucket com o seguinte padrao: metrics_data/[TIPO_DE_TAXI]/arquivo.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdd1552-e499-497d-a0a2-43383748d420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "for trip_type in [\"yellow\", \"green\"]:\n",
    "      relative_path = f\"cleaned_data/{trip_type}/*/*/*\"\n",
    "      df = spark.read.format(\"parquet\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(f\"s3://{bucket}/{relative_path}\")\n",
    "\n",
    "      df = (\n",
    "      df.withColumn(\"date\", F.to_date(F.col(\"tpep_pickup_datetime\")))\n",
    "            .withColumn(\"hour\", F.hour(F.col(\"tpep_pickup_datetime\")))\n",
    "      )\n",
    "\n",
    "      df = df.groupBy(\"date\", \"hour\").agg(\n",
    "      F.sum(\"total_amount\").alias(\"sum_total_amount\").cast(\"double\"),\n",
    "      F.sum(\"passenger_count\").alias(\"sum_passenger_count\").cast(\"int\"),\n",
    "      F.count(\"*\").alias(\"total_rides\").cast(\"int\")\n",
    "      )\n",
    "\n",
    "      df = (df.withColumn(\"avg_total_amounts\", F.col(\"sum_total_amount\") / F.col(\"total_rides\"))\n",
    "              .withColumn(\"avg_passenger_count\", F.col(\"sum_passenger_count\") / F.col(\"total_rides\"))\n",
    "      )\n",
    "\n",
    "      s3_path = f\"s3://{bucket}/metrics_data/{trip_type}/{trip_type}_hour_analysis.parquet\"\n",
    "      df.write.parquet(s3_path, mode=\"overwrite\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NY-taxis-case",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

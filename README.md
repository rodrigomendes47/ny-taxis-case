# üöï NY Taxi Data Engineering Case Study

Esta aplica√ß√£o de engenharia de dados tem como objetivo principal a **an√°lise e transforma√ß√£o de dados hist√≥ricos de corridas de t√°xis de Nova York**, seguindo as melhores pr√°ticas e arquitetura de dados moderna.

## üìù Vis√£o Geral do Projeto

O projeto consiste em um notebook de processamento de dados que realiza a **ingest√£o, limpeza, padroniza√ß√£o e an√°lise** dos dados brutos, facilitando a gera√ß√£o de **insights valiosos** para o neg√≥cio e na disponibiliza√ß√£o de dados tratados para consumo.

Os dados brutos s√£o provenientes do reposit√≥rio oficial de dados de t√°xi de Nova York, acess√≠vel atrav√©s do seguinte link:
https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

### ‚öôÔ∏è Arquitetura e Tecnologia

A solu√ß√£o foi desenvolvida em ambiente **Databricks Community**, utilizando um **Amazon S3 bucket** como Data Lake para armazenamento de dados persistente.

Adotou-se a **Arquitetura Medalh√£o (Medallion Architecture)**, que organiza os dados em camadas Bronze (dados brutos e imutaveis), Silver (dados limpos e padronizados) e Gold (dados agregados, transformados e espec√≠ficos para as regras de neg√≥cio, servindo como a fonte principal para relat√≥rios e BI), garantindo a qualidade e o refinamento progressivo dos dados.
<img width="1927" height="1021" alt="image" src="https://github.com/user-attachments/assets/96269180-ff0c-4b48-ab86-8086af7d35e7" />
fonte: Statplace

## üöÄ Execu√ß√£o

Para executar o notebook e o pipeline de processamento de dados, os seguintes requisitos s√£o mandat√≥rios:

### Pr√©-requisitos

1.  Um **AWS S3 Bucket** criado para servir como Data Lake.
2.  Um **usu√°rio AWS IAM** com as permiss√µes necess√°rias para acesso ao S3 e um **Token de Acesso (Access Key e Secret Key)** v√°lido para autentica√ß√£o.

### Depend√™ncias

As bibliotecas e pacotes necess√°rios para a execu√ß√£o do notebook podem ser instalados de duas maneiras:

1.  **Execu√ß√£o da primeira c√©lula** do notebook principal, que cont√©m os comandos de instala√ß√£o necess√°rios.
2.  Utilizando o arquivo `requirements.txt` via **pip**:
    ```bash
    pip install -r requirements.txt
    ```

## üìÇ Estrutura de Sa√≠da (Data Lake Layers)

Ao final da execu√ß√£o do pipeline, tr√™s diret√≥rios principais s√£o criados no S3 Bucket, correspondendo √†s camadas da Arquitetura Medalh√£o:

### 1. `raw-data` (Camada Bronze)

* C√≥pia fiel dos dados brutos extra√≠dos da fonte original.
* Preservar a integridade do dado original sem qualquer transforma√ß√£o ou limpeza.

### 2. `cleaned-data` (Camada Silver)

* Dados tratados, padronizados e limpos (anormalidades e valores nulos removidos).
* **Particionamento:** Os dados desta camada s√£o otimizados para consulta atrav√©s do seguinte esquema de particionamento:
    ```
    cleaned_data/
    ‚îú‚îÄ‚îÄ [yellow/green]/  <- Tipo de t√°xi
    ‚îÇ   ‚îú‚îÄ‚îÄ Ano/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ M√™s/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ arquivo.parquet
    ```

### 3. `metrics-data` (Camada Gold)

* Dados consolidados com a aplica√ß√£o da **Regra de Neg√≥cio (Business Logic)**.
* Gera√ß√£o de insights e m√©tricas prontas para consumo pelo Business Intelligence (BI) ou aplica√ß√µes de Data Science.
* **M√©trica levantada:** An√°lise da **distribui√ß√£o de corridas por hora**, visando compreender padr√µes de comportamento e tend√™ncias ao longo do tempo.
